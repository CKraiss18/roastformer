# RoastFormer Training & Experiment Plan
**Created**: November 13, 2025
**Purpose**: Strategic roadmap for capstone completion (rubric-optimized)
**Target Score**: 105-115/125 points (84-92%)

---

## üéØ Strategic Insight

**RUBRIC PRIORITY BREAKDOWN:**
- Methodology (50 pts) - ‚úÖ DRAFTED (METHODOLOGY_COURSE_CONNECTIONS.md)
- Implementation & Demo (20 pts) - ‚ö†Ô∏è EXPERIMENTS NEEDED
- Evaluation Framework (15 pts) - ‚ö†Ô∏è WRITEUP NEEDED
- **Total Core Points**: 85/125 (68%)

**Key Insight**: We need **working experiments**, not perfect results, to secure implementation points. Focus on breadth of coverage (positional encodings, conditioning) rather than depth of tuning.

---

## üìä Experiment Tier System

### **TIER 1: MUST DO** (Secures 105/125 pts = 84%)

These experiments directly map to rubric requirements and course connections.

---

#### **Experiment 1: Baseline Model with Sinusoidal Positional Encoding**

**Timeline**: Nov 13-14 (Wed-Thu)
**GPU Time**: 30-60 minutes
**Priority**: üî• CRITICAL

**Configuration:**
```python
config = {
    # Model architecture
    'd_model': 256,              # Medium model (from course Week 5)
    'nhead': 8,                  # Multi-head attention
    'num_layers': 6,             # Transformer decoder layers
    'dim_feedforward': 1024,     # FFN hidden dimension
    'embed_dim': 32,             # Categorical embedding size
    'dropout': 0.1,              # Regularization (Week 8)
    'positional_encoding': 'sinusoidal',  # Vaswani et al. 2017

    # Training hyperparameters
    'batch_size': 8,             # Small dataset regime
    'num_epochs': 100,           # With early stopping
    'learning_rate': 1e-4,       # AdamW default
    'weight_decay': 0.01,        # L2 regularization
    'grad_clip': 1.0,            # Gradient clipping
    'early_stopping_patience': 15,
    'max_sequence_length': 800,  # ~13 minutes max

    # System
    'device': 'cuda',
    'preprocessed_dir': 'preprocessed_data',
    'checkpoint_dir': 'checkpoints',
    'results_dir': 'results',
    'save_every': 10
}
```

**What to Collect:**
1. ‚úÖ `training_summary.txt` (auto-generated by Colab)
2. ‚úÖ Training curves PNG (loss over epochs)
3. ‚úÖ Best model checkpoint (`best_transformer_model.pt`)
4. ‚úÖ Metrics:
   - Final train loss
   - Final validation loss
   - Best validation loss
   - Epoch of best model

**What to Share with Claude:**
```markdown
## Baseline Results (Sinusoidal PE)

**Configuration**: d_model=256, layers=6, heads=8, sinusoidal PE

**Training Progress**:
- Epochs trained: X/100
- Best val loss: X.XX¬∞F (epoch Y)
- Final train loss: X.XX¬∞F
- Final val loss: X.XX¬∞F

**Observations**:
- [Did it converge? Overfit? Underfit?]
- [Any issues during training?]
- [How do generated profiles look?]

**Files**:
- training_summary.txt (attached)
- training_curves.png (attached)
```

**Purpose**:
- Secures **Implementation & Demo (20 pts)**
- Provides baseline for comparisons
- Demonstrates course concept: Transformer architecture (Week 5)

---

#### **Experiment 2: Learned Positional Encoding Comparison**

**Timeline**: Nov 15-16 (Fri-Sat)
**GPU Time**: 30-60 minutes
**Priority**: üî• HIGH

**Configuration:**
```python
# SAME as Experiment 1, except:
config['positional_encoding'] = 'learned'
```

**What to Collect:**
1. ‚úÖ Training summary and curves (same as Exp 1)
2. ‚úÖ Comparison table:

| Metric | Sinusoidal PE | Learned PE | Winner |
|--------|--------------|------------|--------|
| Best val loss | X.XX¬∞F | X.XX¬∞F | ? |
| Convergence speed | X epochs | X epochs | ? |
| Final train loss | X.XX¬∞F | X.XX¬∞F | ? |
| Overfit tendency | Low/Med/High | Low/Med/High | ? |

**What to Share with Claude:**
```markdown
## Positional Encoding Ablation

**Hypothesis** (from Week 5):
- Sinusoidal: Better generalization, deterministic
- Learned: Better fit to training data, more parameters

**Results**:
- Winner: [Sinusoidal/Learned] based on [metric]
- Key difference: [e.g., "Learned PE converged 10 epochs faster but showed more overfitting"]
- Recommendation: [Which to use for final model?]

**Course Connection**:
This directly tests positional encoding theory from Week 5 (Vaswani et al., 2017). Results show [insight about PE choice for time-series vs language].
```

**Purpose**:
- Demonstrates **methodology** - testing course concepts (Week 5)
- Shows **critical thinking** - comparing approaches with trade-offs
- Supports **evaluation framework** (15 pts) - ablation study methodology

---

#### **Experiment 3: Evaluation Framework Writeup**

**Timeline**: Nov 16-17 (Weekend)
**Time**: 3-4 hours (with Claude's help)
**Priority**: üî• HIGH (15 pts)

**What to Write** (create `EVALUATION_FRAMEWORK.md`):

**Section 1: Metric Choices**
- MAE (Mean Absolute Error)
  - Why: Interpretable (¬∞F), directly measures accuracy
  - Trade-off: Doesn't capture shape similarity
  - Course connection: Standard regression metric (Week 9)

- DTW (Dynamic Time Warping)
  - Why: Captures trajectory shape, handles phase shifts
  - Trade-off: More complex, can hide timing errors
  - Course connection: Time-series similarity (Week 10)

- Finish Temperature Accuracy
  - Why: Task-specific (hit target roast level)
  - Trade-off: Ignores profile trajectory
  - Course connection: Task-oriented evaluation (Week 9)

- Physics Constraint Compliance
  - Why: Domain validity (roasting physics)
  - Trade-off: Binary, doesn't measure quality
  - Course connection: Inductive bias validation (Week 2)

**Section 2: Evaluation Protocol**
1. Train/val split strategy (stratified by origin)
2. How to assess generalization (unseen origins)
3. Visual inspection process
4. Attention pattern analysis

**Section 3: Limitations & Future Evaluation**
- Small dataset (28-36 profiles) limits statistical power
- Single roaster (Onyx) - generalization unknown
- No human evaluation (would require roaster feedback)
- With more resources: multi-roaster validation, blind taste tests

**Purpose**:
- Secures **Assessment & Evaluation (15 pts)**
- Shows understanding of trade-offs
- Demonstrates critical thinking

---

### **TIER 2: NICE TO HAVE** (Adds depth, polish, insights)

These experiments enhance presentation and critical analysis but aren't required for core points.

---

#### **Experiment 4: Attention Visualization** (OPTIONAL)

**Timeline**: Nov 17-18 (Sun-Mon) IF TIME
**Time**: 2-3 hours
**Priority**: ‚≠ê MEDIUM (enhances presentation)

**What to Do:**
1. Load best trained model
2. Run forward pass on ONE validation profile
3. Extract attention weights from layer 3 (middle layer)
4. Create heatmap: timesteps √ó timesteps
5. Overlay roast phases (drying, Maillard, development)

**Code Sketch:**
```python
import matplotlib.pyplot as plt
import seaborn as sns

# Load model and profile
model.eval()
with torch.no_grad():
    # Get attention weights (hook into layer 3)
    attn_weights = model.transformer_layers[2].self_attn.attention_weights

# Plot
plt.figure(figsize=(10, 8))
sns.heatmap(attn_weights[0, 0].cpu().numpy(),  # Head 0
            cmap='viridis',
            xticklabels=50, yticklabels=50)
plt.title('Attention Pattern: Head 0, Layer 3')
plt.xlabel('Attended Position (seconds)')
plt.ylabel('Query Position (seconds)')

# Add phase boundaries
plt.axhline(y=240, color='red', linestyle='--', label='End of Drying')
plt.axhline(y=480, color='orange', linestyle='--', label='First Crack')
plt.savefig('attention_visualization.png', dpi=150)
```

**What to Share with Claude:**
- Single attention heatmap image
- 2-3 sentence interpretation:
  ```
  "Attention pattern shows strong diagonal (local context) with
  horizontal bands around first crack (480s), suggesting model
  attends globally during phase transitions. This aligns with
  roasting theory where first crack triggers heating adjustments."
  ```

**Purpose**:
- Great visual for **Presentation (10 pts)** - Visual Aids & Demonstrations
- Supports **Critical Analysis (10 pts)** - "What does this reveal?"
- Shows model is learning physically meaningful patterns

---

#### **Experiment 5: Flavor Conditioning Ablation** (OPTIONAL)

**Timeline**: Nov 18-19 (Mon-Tue) IF TIME
**GPU Time**: 1 hour
**Priority**: ‚≠ê MEDIUM (tests novel contribution)

**What to Do:**
Train model WITHOUT flavor features:
```python
# Modify conditioning module to exclude flavors
# In transformer_adapter.py or data loader:
# - Remove flavor encoding
# - Only use: origin, process, roast_level, variety, altitude, etc.
```

**Comparison:**
| Metric | With Flavors (Baseline) | Without Flavors | Difference |
|--------|------------------------|-----------------|------------|
| Val loss | X.XX¬∞F | X.XX¬∞F | ŒîX.XX¬∞F |
| MAE | X.XX¬∞F | X.XX¬∞F | ŒîX.XX¬∞F |

**What to Share with Claude:**
```markdown
## Novel Contribution Test: Flavor Conditioning

**Hypothesis**: Flavor features (berries, chocolate, etc.) provide
additional signal for profile generation beyond physical bean properties.

**Results**:
- Including flavors [improved/didn't improve] MAE by X.X¬∞F
- [Interpretation: flavors capture underlying chemistry OR just correlate with origin]

**Insight**: [Does this validate the novel contribution claim?]
```

**Purpose**:
- Tests your **novel contribution** (flavor-guided generation)
- Supports **Critical Analysis (10 pts)** - "What is the impact?"
- Validates whether flavor encoding was worth the complexity

---

## ‚ùå What NOT to Do (Low Rubric ROI)

Based on rubric analysis, **SKIP these** unless you have extra time:

### **Skip: Model Size Ablation (128, 256, 512)**
- **Why**: Rubric doesn't reward extensive hyperparameter tuning
- **ROI**: Low (minimal additional points)
- **Decision**: One working model = full 20 pts

### **Skip: RoPE Positional Encoding**
- **Why**: More complex implementation, diminishing returns
- **ROI**: Medium (interesting but time-consuming)
- **Decision**: Sinusoidal vs Learned is sufficient for PE comparison

### **Skip: Extensive Hyperparameter Tuning**
- **Why**: Not in rubric, doesn't add points
- **ROI**: Very low
- **Decision**: Baseline config is good enough

### **Skip: Multiple Loss Functions (MSE, MAE, Huber)**
- **Why**: One working loss function is sufficient
- **ROI**: Low
- **Decision**: MSE is standard for regression

### **Skip: Data Augmentation Experiments**
- **Why**: Interesting but tangential
- **ROI**: Low (unless training fails completely)
- **Decision**: Save for "future work" discussion

---

## üì¶ Deliverables Checklist

### **After Experiment 1 (Baseline):**
- [ ] `training_summary.txt`
- [ ] `training_curves.png`
- [ ] `best_transformer_model.pt`
- [ ] Share results with Claude (format above)

### **After Experiment 2 (Learned PE):**
- [ ] `training_summary_learned_pe.txt`
- [ ] `training_curves_learned_pe.png`
- [ ] Comparison table (markdown or CSV)
- [ ] Share ablation analysis with Claude

### **After Experiment 3 (Evaluation Writeup):**
- [ ] `EVALUATION_FRAMEWORK.md` (draft)
- [ ] Share with Claude for review/polish

### **Optional (Tier 2):**
- [ ] `attention_visualization.png` (Exp 4)
- [ ] Flavor ablation results (Exp 5)

---

## üìÖ Timeline Summary

### **Week 1: Core Experiments (Nov 13-17)**
| Day | Task | Deliverable | Points Secured |
|-----|------|-------------|----------------|
| **Wed 11/13** | Baseline training (Exp 1) | Training results | 20 pts (Implementation) |
| **Thu 11/14** | Analyze baseline, share with Claude | Results summary | - |
| **Fri 11/15** | Learned PE training (Exp 2) | Ablation results | - |
| **Sat 11/16** | Draft evaluation framework (Exp 3) | EVALUATION_FRAMEWORK.md | 15 pts (Evaluation) |
| **Sun 11/17** | Polish evaluation, optional: attention viz | Final draft + optional viz | - |

**Week 1 Target**: 85/125 points secured (68%)

---

### **Week 2: Analysis & Presentation (Nov 18-22)**
| Day | Task | Deliverable | Points Secured |
|-----|------|-------------|----------------|
| **Mon 11/18** | Optional: Flavor ablation (Exp 5) | Ablation results | - |
| **Tue 11/19** | Critical analysis writeup | CRITICAL_ANALYSIS.md | 10 pts |
| **Wed 11/20** | Presentation slides (structure) | Draft slides | - |
| **Thu 11/21** | Visual aids (architecture diagram, plots) | Presentation visuals | - |
| **Fri 11/22** | Presentation polish, practice | Final slides | 10 pts (Presentation) |

**Week 2 Target**: 105/125 points secured (84%)

---

### **Week 3: Polish & Practice (Nov 25-29)**
| Day | Task | Deliverable | Points Secured |
|-----|------|-------------|----------------|
| **Mon 11/25** | Model card, documentation polish | MODEL_CARD.md | 5 pts (Model Card) |
| **Tue 11/26** | Presentation rehearsal | - | - |
| **Wed 11/27** | Final presentation practice | - | - |
| **Thu 11/28** | Thanksgiving break | - | - |
| **Fri 11/29** | Final prep, backup materials | Everything ready | 5 pts (Documentation) |

**Week 3 Target**: 115-120/125 points secured (92-96%)

---

## üéØ Success Criteria by Experiment

### **Experiment 1 (Baseline) - Minimum Success:**
- ‚úÖ Model trains without errors
- ‚úÖ Loss decreases over epochs (not flat/diverging)
- ‚úÖ Generated profiles have reasonable temperature ranges (350-450¬∞F)
- ‚úÖ Can load checkpoint and generate new profiles

### **Experiment 1 (Baseline) - Good Success:**
- ‚úÖ Validation loss < 15¬∞F
- ‚úÖ Generated profiles visually resemble real ones
- ‚úÖ >80% compliance with physics constraints (monotonicity, RoR)
- ‚úÖ Model reaches early stopping before epoch 100

### **Experiment 1 (Baseline) - Great Success:**
- ‚úÖ Validation loss < 10¬∞F
- ‚úÖ MAE < 5¬∞F on test profiles
- ‚úÖ >95% physics compliance
- ‚úÖ Attention patterns interpretable

**Remember**: "Good Success" is sufficient for full 20 pts. "Great Success" doesn't add points, just makes presentation more impressive.

---

### **Experiment 2 (Learned PE) - Success Criteria:**
- ‚úÖ Both models train successfully
- ‚úÖ Can compare performance quantitatively (loss, MAE)
- ‚úÖ Can explain trade-offs (convergence speed vs overfitting)
- ‚úÖ Can recommend one approach with justification

**Key**: It's okay if learned PE is worse! The point is demonstrating you tested course concepts and can analyze results.

---

## üìä What to Share with Claude After Each Run

Use this template for efficient communication:

```markdown
## [Experiment Name] Results

**Date**: [Date]
**Configuration**:
- d_model: X
- layers: X
- positional_encoding: [type]
- [other key params]

**Training Summary**:
- Total epochs: X/100
- Best epoch: X
- Best val loss: X.XX¬∞F
- Final train loss: X.XX¬∞F
- Final val loss: X.XX¬∞F
- Training time: X minutes

**Metrics**:
| Metric | Value |
|--------|-------|
| MAE | X.X¬∞F |
| DTW distance | X.X |
| Finish temp accuracy | X% within 10¬∞F |
| Monotonicity compliance | X% |
| RoR compliance | X% |

**Observations**:
- [Did it converge well?]
- [Overfit? Underfit?]
- [Generated profiles look good/bad/mediocre?]
- [Any issues or surprises?]

**Files Attached**:
- training_summary.txt
- training_curves.png
- [optional: example_profile.png]

**Questions for Claude**:
- [Any specific questions about results?]
- [Help needed with interpretation?]
- [Next steps unclear?]
```

---

## üí° Claude's Role in This Process

### **I'll Help You With:**

1. **Interpreting Results**
   - "Is this validation loss good for small dataset?"
   - "Why did learned PE overfit more?"
   - "What does this attention pattern mean?"

2. **Writing Evaluation Framework** (15 pts)
   - Structuring the document
   - Explaining metric trade-offs
   - Connecting to course concepts
   - Polishing language

3. **Critical Analysis** (10 pts)
   - "What is the impact?" ‚Üí Help articulate practical value
   - "What does it reveal?" ‚Üí Connect results to theory
   - "What's next?" ‚Üí Suggest future directions

4. **Presentation Materials** (10 pts)
   - Slide structure recommendations
   - Visual aid suggestions
   - Demo flow planning
   - Backup plan for tech issues

5. **Connecting Results to Course Theory**
   - Linking experiments back to Week X lectures
   - Citing relevant papers (Vaswani et al., etc.)
   - Polishing methodology document

### **You'll Handle:**
- Running training in Colab
- Collecting results files
- Generating example profiles
- Taking screenshots/exporting plots

---

## üö® Risk Management

### **High-Risk Scenarios & Mitigations:**

#### **Risk 1: Baseline doesn't train (diverges/NaN loss)**
**Mitigation**:
- Reduce learning rate to 5e-5
- Increase gradient clipping to 0.5
- Check for NaN in data
- Share error logs with Claude immediately

#### **Risk 2: Results are mediocre (val loss ~20¬∞F)**
**Mitigation**:
- **THIS IS OKAY!** Rubric doesn't require perfect results
- Focus on explanation: "Small dataset (144 profiles) limits performance"
- Emphasize methodology over results
- Suggest improvements in "future work"

#### **Risk 3: Run out of time for Tier 2 experiments**
**Mitigation**:
- Skip Tier 2 entirely
- Focus on evaluation writeup and presentation
- 85 points (Tier 1 only) is still B+ grade

#### **Risk 4: Colab disconnects during training**
**Mitigation**:
- Save checkpoints every 10 epochs (already in config)
- Can resume from checkpoint if needed
- Use Colab Pro ($10) for longer runtime if necessary

---

## üìö Reference Material Locations

### **Course Connections:**
- `METHODOLOGY_COURSE_CONNECTIONS.md` - Full course theory mappings
- `RUBRIC_STRATEGY_AND_TRACKING.md` - Point optimization strategy

### **Implementation:**
- `train_transformer.py` - Main training script
- `src/model/transformer_adapter.py` - Model architecture
- `src/dataset/preprocessed_data_loader.py` - Data loading
- `RoastFormer_Colab_Training.ipynb` - Colab notebook

### **Capstone Requirements:**
- `project_rubric.pdf` - Official grading criteria
- `CLAUDE.md` - Overall project guide

---

## ‚úÖ Pre-Flight Checklist

Before starting Experiment 1, verify:

- [ ] Colab has GPU enabled (Runtime ‚Üí Change runtime type ‚Üí GPU)
- [ ] Data extracted successfully to `/content/` (144 profiles)
- [ ] `preprocessed_data/` folder exists with all files
- [ ] Config parameters set (cell 9 in Colab notebook)
- [ ] Notebook cells run in order without errors
- [ ] Enough time for 30-60 min training run

---

## üéØ Final Reminder: Rubric Over Perfection

From your rubric doc:

> **"The rubric rewards understanding and communication over implementation perfection."**
>
> **"A simple model with excellent explanation beats a complex model with weak theory."**

### What This Means for Experiments:

‚úÖ **DO**:
- Run 2-3 core experiments (baseline + 1-2 ablations)
- Collect clean results and metrics
- Explain choices and trade-offs
- Connect to course concepts
- Analyze what worked and what didn't

‚ùå **DON'T**:
- Chase perfect validation loss
- Run dozens of hyperparameter combinations
- Spend days debugging minor improvements
- Implement complex features (RoPE, multi-modal attention, etc.)
- Worry if results are mediocre

**Target**: 2-3 solid experiments with thoughtful analysis = 105-115 points (84-92%)

---

## üìû How to Use This Document

### **As You Work:**
1. Read the experiment section before starting
2. Use the config exactly as specified
3. Collect the listed deliverables
4. Share results with Claude using the template
5. Move to next experiment

### **If Session Times Out:**
1. Re-read this document
2. Check where you are in timeline
3. Continue from that point
4. Share new results when Claude is available

### **Quick Reference:**
- **What to run?** ‚Üí See Experiment 1, 2, 3 configs
- **What to share?** ‚Üí See "What to Share with Claude" sections
- **What if stuck?** ‚Üí See "Risk Management" section
- **Am I on track?** ‚Üí Check timeline vs. current date

---

**Last Updated**: November 13, 2025
**Status**: Ready to begin Experiment 1 (Baseline)
**Next Action**: Train baseline model in Colab with sinusoidal PE

---

**Good luck! This plan is optimized for maximum rubric points with minimum time investment. Stay focused on Tier 1 experiments and remember: working demo > perfect results. üöÄ**
