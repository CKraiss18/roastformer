{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üöÄ RoastFormer Transformer Training - Google Colab\n",
        "\n",
        "**Complete training pipeline for coffee roast profile generation**\n",
        "\n",
        "Author: Charlee Kraiss  \n",
        "Project: RoastFormer - Transformer-Based Roast Profile Generation  \n",
        "Date: November 2024\n",
        "\n",
        "---\n",
        "\n",
        "## üìã What This Notebook Does\n",
        "\n",
        "1. ‚úÖ Sets up GPU environment\n",
        "2. ‚úÖ Uploads your preprocessed data\n",
        "3. ‚úÖ Trains the full transformer\n",
        "4. ‚úÖ Saves results & checkpoints\n",
        "5. ‚úÖ Generates downloadable results package\n",
        "\n",
        "**Estimated Runtime:** 30-60 minutes (with free T4 GPU)\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Quick Start\n",
        "\n",
        "1. **Runtime ‚Üí Change runtime type ‚Üí GPU (T4)**\n",
        "2. Run cells in order\n",
        "3. Upload `roastformer_data.zip` when prompted\n",
        "4. Download results at the end\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1Ô∏è‚É£ Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "check_gpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "770507cd-c47d-49b4-da3b-ec3e772b2b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "GPU CHECK\n",
            "================================================================================\n",
            "CUDA available: True\n",
            "GPU: NVIDIA L4\n",
            "CUDA version: 12.6\n",
            "‚úÖ GPU ready for training!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(\"=\"*80)\n",
        "print(\"GPU CHECK\")\n",
        "print(\"=\"*80)\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(\"‚úÖ GPU ready for training!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No GPU detected. Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "install_deps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43ece8d9-8b33-40cd-f4de-3e51ff770444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dependencies installed\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (if needed)\n",
        "!pip install -q pandas scikit-learn matplotlib\n",
        "\n",
        "print(\"‚úÖ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH7UtqXDYtkH",
        "outputId": "50b2fa94-3ca7-4e18-ebbc-a60625ec62b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/\"Colab Notebooks\"/\"GEN_AI\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfeFDA-JYQFI",
        "outputId": "d092ee16-d675-4d44-c46c-f372bec477f2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Colab Notebooks/GEN_AI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OFtjlhceY-J5",
        "outputId": "8830ded1-50ee-49bf-9d29-47f5fc1dc6bc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/Colab Notebooks/GEN_AI'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLG_1JidZC0r",
        "outputId": "f6ed9d8a-3441-4eda-e7ac-1ffcc4440dab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RoastFormer_Colab_Training.ipynb   roastformer_data_20251111_092727.zip\n",
            "\u001b[0m\u001b[01;34mroastformer_data_20251111_092727\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upload"
      },
      "source": [
        "## 2Ô∏è‚É£ Upload Your Data\n",
        "\n",
        "Upload the `roastformer_data.zip` file created by the packaging script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "upload_data",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9a7cc2d-ebc5-4dc1-b8ce-c4fbaca7be70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "EXTRACTING DATA FROM GOOGLE DRIVE\n",
            "================================================================================\n",
            "‚úÖ Found zip file\n",
            "Working directory: /content\n",
            "\n",
            "üì¶ Extracting...\n",
            "‚úÖ Extraction complete\n",
            "\n",
            "üìÅ Verifying:\n",
            "total 13M\n",
            "-rw-r--r-- 1 root root  267 Nov 13 18:41 dataset_stats.json\n",
            "-rw-r--r-- 1 root root  19K Nov 13 18:41 train_metadata.csv\n",
            "-rw-r--r-- 1 root root  11M Nov 13 18:41 train_profiles.json\n",
            "-rw-r--r-- 1 root root 3.3K Nov 13 18:41 val_metadata.csv\n",
            "-rw-r--r-- 1 root root 1.8M Nov 13 18:41 val_profiles.json\n",
            "\n",
            "üìä Dataset: 144 profiles\n",
            "‚úÖ Ready to train!\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"EXTRACTING DATA FROM GOOGLE DRIVE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "zip_path = '/content/gdrive/MyDrive/Colab Notebooks/GEN_AI/roastformer_data_20251111_092727.zip'\n",
        "\n",
        "if os.path.exists(zip_path):\n",
        "    print(f\"‚úÖ Found zip file\")\n",
        "\n",
        "    # KEY FIX: Change to /content first\n",
        "    os.chdir('/content')\n",
        "    print(f\"Working directory: {os.getcwd()}\")\n",
        "\n",
        "    print(f\"\\nüì¶ Extracting...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall('.')  # Extract to current directory\n",
        "\n",
        "    print(\"‚úÖ Extraction complete\")\n",
        "\n",
        "    # Verify\n",
        "    print(\"\\nüìÅ Verifying:\")\n",
        "    !ls -lh preprocessed_data/\n",
        "\n",
        "    # Show stats\n",
        "    import json\n",
        "    with open('preprocessed_data/dataset_stats.json', 'r') as f:\n",
        "        stats = json.load(f)\n",
        "    print(f\"\\nüìä Dataset: {stats['total_profiles']} profiles\")\n",
        "    print(\"‚úÖ Ready to train!\")\n",
        "else:\n",
        "    print(f\"‚ùå Zip not found at: {zip_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "verify"
      },
      "source": [
        "## 3Ô∏è‚É£ Verify Data Loaded Correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "verify_data",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9e3b738-e7c2-4a9b-849c-bb09774d043f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "DATA VERIFICATION\n",
            "================================================================================\n",
            "‚úÖ preprocessed_data/train_profiles.json\n",
            "‚úÖ preprocessed_data/val_profiles.json\n",
            "‚úÖ preprocessed_data/train_metadata.csv\n",
            "‚úÖ preprocessed_data/val_metadata.csv\n",
            "‚úÖ preprocessed_data/dataset_stats.json\n",
            "‚úÖ src/dataset/preprocessed_data_loader.py\n",
            "‚úÖ src/model/transformer_adapter.py\n",
            "‚úÖ train_transformer.py\n",
            "\n",
            "‚úÖ All files present!\n",
            "\n",
            "üìä Dataset Statistics:\n",
            "   Total profiles: 144\n",
            "   Training: 123\n",
            "   Validation: 21\n",
            "   Unique origins: 18\n",
            "   Unique processes: 13\n",
            "   Unique varieties: 24\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DATA VERIFICATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Check structure\n",
        "expected_files = [\n",
        "    'preprocessed_data/train_profiles.json',\n",
        "    'preprocessed_data/val_profiles.json',\n",
        "    'preprocessed_data/train_metadata.csv',\n",
        "    'preprocessed_data/val_metadata.csv',\n",
        "    'preprocessed_data/dataset_stats.json',\n",
        "    'src/dataset/preprocessed_data_loader.py',\n",
        "    'src/model/transformer_adapter.py',\n",
        "    'train_transformer.py'\n",
        "]\n",
        "\n",
        "all_good = True\n",
        "for filepath in expected_files:\n",
        "    exists = os.path.exists(filepath)\n",
        "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
        "    print(f\"{status} {filepath}\")\n",
        "    if not exists:\n",
        "        all_good = False\n",
        "\n",
        "if all_good:\n",
        "    print(\"\\n‚úÖ All files present!\")\n",
        "\n",
        "    # Load dataset stats\n",
        "    with open('preprocessed_data/dataset_stats.json', 'r') as f:\n",
        "        stats = json.load(f)\n",
        "\n",
        "    print(\"\\nüìä Dataset Statistics:\")\n",
        "    print(f\"   Total profiles: {stats['total_profiles']}\")\n",
        "    print(f\"   Training: {stats['train_size']}\")\n",
        "    print(f\"   Validation: {stats['val_size']}\")\n",
        "    print(f\"   Unique origins: {stats['unique_origins']}\")\n",
        "    print(f\"   Unique processes: {stats['unique_processes']}\")\n",
        "    print(f\"   Unique varieties: {stats['unique_varieties']}\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Some files missing! Please re-upload the data package.\")\n",
        "\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config"
      },
      "source": [
        "## 4Ô∏è‚É£ Configure Training\n",
        "\n",
        "Choose your model configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "training_config",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac9bcafd-2eb2-43ae-d7dd-3d66f092a05f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "TRAINING CONFIGURATION\n",
            "================================================================================\n",
            "  d_model: 256\n",
            "  nhead: 8\n",
            "  num_layers: 6\n",
            "  dim_feedforward: 1024\n",
            "  embed_dim: 32\n",
            "  dropout: 0.1\n",
            "  positional_encoding: sinusoidal\n",
            "  batch_size: 8\n",
            "  num_epochs: 100\n",
            "  learning_rate: 0.0001\n",
            "  weight_decay: 0.01\n",
            "  grad_clip: 1.0\n",
            "  early_stopping_patience: 15\n",
            "  max_sequence_length: 800\n",
            "  device: cuda\n",
            "  preprocessed_dir: preprocessed_data\n",
            "  checkpoint_dir: checkpoints\n",
            "  results_dir: results\n",
            "  save_every: 10\n",
            "================================================================================\n",
            "\n",
            "üìä Estimated model size: ~10M\n",
            "‚è±Ô∏è  Estimated training time: 30-60 min (on GPU)\n"
          ]
        }
      ],
      "source": [
        "# Training Configuration\n",
        "# Modify these parameters as needed\n",
        "\n",
        "config = {\n",
        "    # Model architecture\n",
        "    'd_model': 256,              # Model dimension (128=small, 256=medium, 512=large)\n",
        "    'nhead': 8,                  # Attention heads\n",
        "    'num_layers': 6,             # Transformer layers\n",
        "    'dim_feedforward': 1024,     # FFN dimension\n",
        "    'embed_dim': 32,             # Categorical embedding size\n",
        "    'dropout': 0.1,              # Dropout rate\n",
        "    'positional_encoding': 'sinusoidal',  # 'sinusoidal' or 'learned'\n",
        "\n",
        "    # Training hyperparameters\n",
        "    'batch_size': 8,             # Batch size (4-16 for small dataset)\n",
        "    'num_epochs': 100,           # Number of epochs\n",
        "    'learning_rate': 1e-4,       # Learning rate\n",
        "    'weight_decay': 0.01,        # L2 regularization\n",
        "    'grad_clip': 1.0,            # Gradient clipping\n",
        "    'early_stopping_patience': 15,  # Early stopping patience\n",
        "    'max_sequence_length': 800,  # Max profile length\n",
        "\n",
        "    # System\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    'preprocessed_dir': 'preprocessed_data',\n",
        "    'checkpoint_dir': 'checkpoints',\n",
        "    'results_dir': 'results',\n",
        "    'save_every': 10             # Save checkpoint every N epochs\n",
        "}\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TRAINING CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "for key, value in config.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Estimate parameters\n",
        "if config['d_model'] == 128:\n",
        "    params = \"~2M\"\n",
        "    time_est = \"15-30 min\"\n",
        "elif config['d_model'] == 256:\n",
        "    params = \"~10M\"\n",
        "    time_est = \"30-60 min\"\n",
        "else:\n",
        "    params = \"~40M\"\n",
        "    time_est = \"1-2 hours\"\n",
        "\n",
        "print(f\"\\nüìä Estimated model size: {params}\")\n",
        "print(f\"‚è±Ô∏è  Estimated training time: {time_est} (on GPU)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train"
      },
      "source": [
        "## 5Ô∏è‚É£ Train the Transformer\n",
        "\n",
        "**This cell will take 30-60 minutes with GPU.**\n",
        "\n",
        "You can monitor progress in real-time below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_training"
      },
      "outputs": [],
      "source": [
        "# Import training script\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "from train_transformer import TransformerTrainer\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Device: {config['device']}\")\n",
        "print(f\"Epochs: {config['num_epochs']}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = TransformerTrainer(config)\n",
        "\n",
        "# Load data\n",
        "trainer.load_data()\n",
        "\n",
        "# Initialize model\n",
        "trainer.initialize_model()\n",
        "\n",
        "# Train!\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ TRAINING COMPLETE!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "results"
      },
      "source": [
        "## 6Ô∏è‚É£ Generate Results Summary\n",
        "\n",
        "Create a comprehensive summary of training results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generate_summary"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "# Load training results\n",
        "with open('results/transformer_training_results.json', 'r') as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TRAINING RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nüìä Model Configuration:\")\n",
        "print(f\"   d_model: {results['config']['d_model']}\")\n",
        "print(f\"   Layers: {results['config']['num_layers']}\")\n",
        "print(f\"   Heads: {results['config']['nhead']}\")\n",
        "print(f\"   Parameters: {results['num_parameters']:,}\")\n",
        "\n",
        "print(f\"\\nüìà Training Progress:\")\n",
        "print(f\"   Final epoch: {results['final_epoch']}/{results['config']['num_epochs']}\")\n",
        "print(f\"   Best val loss: {results['best_val_loss']:.4f}¬∞F\")\n",
        "print(f\"   Final train loss: {results['train_losses'][-1]:.4f}¬∞F\")\n",
        "print(f\"   Final val loss: {results['val_losses'][-1]:.4f}¬∞F\")\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(results['train_losses'], label='Train Loss', linewidth=2)\n",
        "plt.plot(results['val_losses'], label='Val Loss', linewidth=2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (¬∞F)')\n",
        "plt.title('Training Progress')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(results['train_losses'], label='Train Loss', linewidth=2)\n",
        "plt.plot(results['val_losses'], label='Val Loss', linewidth=2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (¬∞F)')\n",
        "plt.title('Training Progress (Log Scale)')\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/training_curves.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Training curves saved to results/training_curves.png\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "package"
      },
      "source": [
        "## 7Ô∏è‚É£ Package Results for Download\n",
        "\n",
        "Create a downloadable package with all results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "package_results"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Create results package\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "package_name = f'roastformer_results_{timestamp}.zip'\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PACKAGING RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "with zipfile.ZipFile(package_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    # Add checkpoint\n",
        "    zipf.write('checkpoints/best_transformer_model.pt',\n",
        "               'best_transformer_model.pt')\n",
        "    print(\"‚úÖ Added: best_transformer_model.pt\")\n",
        "\n",
        "    # Add results\n",
        "    zipf.write('results/transformer_training_results.json',\n",
        "               'transformer_training_results.json')\n",
        "    print(\"‚úÖ Added: transformer_training_results.json\")\n",
        "\n",
        "    # Add training curves\n",
        "    zipf.write('results/training_curves.png',\n",
        "               'training_curves.png')\n",
        "    print(\"‚úÖ Added: training_curves.png\")\n",
        "\n",
        "    # Create a summary text file\n",
        "    summary = f\"\"\"RoastFormer Training Results Summary\n",
        "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "MODEL CONFIGURATION\n",
        "-------------------\n",
        "d_model: {results['config']['d_model']}\n",
        "Layers: {results['config']['num_layers']}\n",
        "Heads: {results['config']['nhead']}\n",
        "Parameters: {results['num_parameters']:,}\n",
        "Positional Encoding: {results['config']['positional_encoding']}\n",
        "\n",
        "TRAINING RESULTS\n",
        "----------------\n",
        "Epochs Trained: {results['final_epoch']}\n",
        "Best Validation Loss: {results['best_val_loss']:.4f}¬∞F\n",
        "Final Train Loss: {results['train_losses'][-1]:.4f}¬∞F\n",
        "Final Val Loss: {results['val_losses'][-1]:.4f}¬∞F\n",
        "\n",
        "DATASET\n",
        "-------\n",
        "Total Profiles: {results['feature_dims']['num_origins']} origins\n",
        "Origins: {results['feature_dims']['num_origins']}\n",
        "Processes: {results['feature_dims']['num_processes']}\n",
        "Varieties: {results['feature_dims']['num_varieties']}\n",
        "Flavors: {results['feature_dims']['num_flavors']}\n",
        "\n",
        "FILES INCLUDED\n",
        "--------------\n",
        "1. best_transformer_model.pt - Best model checkpoint\n",
        "2. transformer_training_results.json - Complete results\n",
        "3. training_curves.png - Training visualization\n",
        "4. training_summary.txt - This file\n",
        "\n",
        "TO USE THESE RESULTS\n",
        "--------------------\n",
        "1. Download this zip file\n",
        "2. Extract to your RoastFormer project\n",
        "3. Share training_summary.txt with Claude\n",
        "4. Use evaluate_transformer.py to analyze the model\n",
        "5. Use generate_profiles.py to create new profiles\n",
        "\"\"\"\n",
        "\n",
        "    zipf.writestr('training_summary.txt', summary)\n",
        "    print(\"‚úÖ Added: training_summary.txt\")\n",
        "\n",
        "print(f\"\\nüì¶ Package created: {package_name}\")\n",
        "print(f\"   Size: {os.path.getsize(package_name) / 1024 / 1024:.2f} MB\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download"
      },
      "source": [
        "## 8Ô∏è‚É£ Download Results\n",
        "\n",
        "Download the complete results package to your Mac."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_results"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DOWNLOAD RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Downloading: {package_name}\")\n",
        "print(\"\\nThis package contains:\")\n",
        "print(\"  ‚Ä¢ Trained model checkpoint\")\n",
        "print(\"  ‚Ä¢ Complete training results (JSON)\")\n",
        "print(\"  ‚Ä¢ Training curves visualization\")\n",
        "print(\"  ‚Ä¢ Summary text file\")\n",
        "print(\"\\nOnce downloaded:\")\n",
        "print(\"  1. Extract the zip file\")\n",
        "print(\"  2. Share 'training_summary.txt' with Claude\")\n",
        "print(\"  3. Move checkpoint to checkpoints/ folder\")\n",
        "print(\"  4. Run evaluation and generation scripts\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "files.download(package_name)\n",
        "\n",
        "print(\"\\n‚úÖ Download complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "next_steps"
      },
      "source": [
        "## üéâ Training Complete!\n",
        "\n",
        "### What You Have Now:\n",
        "\n",
        "1. ‚úÖ **Trained transformer model** - Ready for profile generation\n",
        "2. ‚úÖ **Training results** - Complete metrics and curves\n",
        "3. ‚úÖ **Downloadable package** - Everything you need\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "**On Your Mac:**\n",
        "\n",
        "1. **Extract the results:**\n",
        "   ```bash\n",
        "   cd ~/VANDY/FALL_2025/GEN_AI_THEORY/ROASTFormer\n",
        "   unzip roastformer_results_*.zip\n",
        "   ```\n",
        "\n",
        "2. **Share results with Claude:**\n",
        "   - Open `training_summary.txt`\n",
        "   - Paste contents in chat with Claude\n",
        "   - Claude will analyze and suggest next steps\n",
        "\n",
        "3. **Evaluate the model:**\n",
        "   ```bash\n",
        "   python evaluate_transformer.py --plot --num_samples 10\n",
        "   ```\n",
        "\n",
        "4. **Generate custom profiles:**\n",
        "   ```bash\n",
        "   python generate_profiles.py \\\n",
        "     --origin \"Ethiopia\" \\\n",
        "     --flavors \"berries,floral\" \\\n",
        "     --plot\n",
        "   ```\n",
        "\n",
        "### For Ablation Studies:\n",
        "\n",
        "**Modify the config in cell 4 and re-run:**\n",
        "\n",
        "- Try `positional_encoding: 'learned'`\n",
        "- Try different model sizes (d_model: 128, 256, 512)\n",
        "- Compare results\n",
        "\n",
        "---\n",
        "\n",
        "**Questions?** Share the training summary with Claude for analysis!\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}